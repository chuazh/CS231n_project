{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "def collect_video_framecount(action,subject,trial_num):\n",
    "    \n",
    "    action_dict = {'KT':'Knot_Tying','S':'Suturing','NP': 'Needle_Passing'}\n",
    "    \n",
    "    act = action_dict[action]\n",
    "\n",
    "    filename1 = act+'/video/'+act+'_'+subject+'00'+str(trial_num)+'_capture1.avi'\n",
    "    filename2 = act+'/video/'+act+'_'+subject+'00'+str(trial_num)+'_capture2.avi'\n",
    "    \n",
    "    print('reading '+filename1)\n",
    "    \n",
    "    vidcap1 = cv2.VideoCapture(filename1)\n",
    "    vidcap2 = cv2.VideoCapture(filename2)\n",
    "    \n",
    "    count = 0\n",
    "    success = True\n",
    "    \n",
    "    while success:\n",
    "      success,image = vidcap1.read()\n",
    "    \n",
    "      '''\n",
    "      success,image = vidcap2.read()\n",
    "      '''  \n",
    "      count += 1\n",
    "\n",
    "    print('total frame count : %d' % count)\n",
    "    \n",
    "    return count-1\n",
    "\n",
    "def collect_video_sample(action,subject,trial_num,num_frames):\n",
    "    \n",
    "    action_dict = {'KT':'Knot_Tying','S':'Suturing','NP': 'Needle_Passing'}\n",
    "    \n",
    "    act = action_dict[action]\n",
    "\n",
    "    filename1 = act+'/video/'+act+'_'+subject+'00'+str(trial_num)+'_capture1.avi'\n",
    "    filename2 = act+'/video/'+act+'_'+subject+'00'+str(trial_num)+'_capture2.avi'\n",
    "    \n",
    "    print('reading '+filename1)\n",
    "    \n",
    "    vidcap1 = cv2.VideoCapture(filename1)\n",
    "    vidcap2 = cv2.VideoCapture(filename2)\n",
    "    \n",
    "    # collect kinematic data\n",
    "    filepath = act + '/kinematics/AllGestures/'\n",
    "    filename = filepath + act + '_' +subject + '00' + str(trial_num) + '.txt'\n",
    "    data = np.loadtxt(filename)\n",
    "    num_labels = data.shape[0]\n",
    "    print('total labels loaded: %d' % num_labels)\n",
    "    \n",
    "    if (num_labels>num_frames):\n",
    "          pass\n",
    "    else:\n",
    "          num_frames = num_labels\n",
    "          \n",
    "    count = 0\n",
    "    success = True\n",
    "    \n",
    "    while success and count<num_frames:\n",
    "      success,image = vidcap1.read()\n",
    "      write_name = 'data/' + subject+'_'+str(trial_num)+'_1'+'_%d_'+ action + '.png'\n",
    "      cv2.imwrite(write_name % count, image)     # save frame as png file\n",
    "    \n",
    "      '''\n",
    "      success,image = vidcap2.read()\n",
    "      write_name = 'data/' + subject+'_'+str(trial_num)+'_2'+'_%d_'+ action + '.png'\n",
    "      cv2.imwrite(write_name % count, image)     # save frame as png file\n",
    "      '''\n",
    "      count += 1\n",
    "      if count%100 == 0:\n",
    "          print('capturing frame %d' % count)  \n",
    "    \n",
    "    print('total frame count : %d' % count)\n",
    "    \n",
    "    # only take cols 38-49 (slave left) and 57-68 (slave right)\n",
    "    \n",
    "    dataL = data[:count,38:50]\n",
    "    dataR = data[:count,57:69]\n",
    "    \n",
    "    out = np.hstack((dataL,dataR))\n",
    "    \n",
    "    print('total labels saved: %d' % out.shape[0])\n",
    "    \n",
    "    return out\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_list = {}\n",
    "subject_list['KT'] = [['B','C','D','E','F','G','H','I'],[[0,1,2,3],[0,1,2,3,4],[0,1,2,3,4],[0,1,2,3,4],[0,1,2,3,4],[0,1,2,3,4],[2,3,4],[0,1,2,4]]]\n",
    "#subject_list['KT'] = [['B'],[4]]\n",
    "\n",
    "\n",
    "\n",
    "y = np.empty((0,24))\n",
    "\n",
    "task = 'KT'\n",
    "subj = subject_list[task][0]\n",
    "trial = subject_list[task][1]\n",
    "for i in range(len(subj)):\n",
    "    for j in trial[i]:\n",
    "        num_frames = collect_video_framecount(task,subj[i],j+1)\n",
    "        out = collect_video_sample(task,subj[i],j+1,num_frames)\n",
    "        y = np.vstack((y,out))\n",
    "\n",
    "import pickle\n",
    "\n",
    "print(y.shape)\n",
    "picklefile = open('kinematics', 'wb') \n",
    "pickle.dump(y,picklefile)\n",
    "picklefile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GOT THIS ONLINE, NEED TO FIGURE OUT HOW TO CITE IT https://www.lfd.uci.edu/~gohlke/code/transformations.py.html\n",
    "\n",
    "def quaternion_from_matrix(matrix, isprecise=False):\n",
    "    \"\"\"Return quaternion from rotation matrix.\n",
    "\n",
    "    If isprecise is True, the input matrix is assumed to be a precise rotation\n",
    "    matrix and a faster algorithm is used.\n",
    "\n",
    "    >>> q = quaternion_from_matrix(numpy.identity(4), True)\n",
    "    >>> numpy.allclose(q, [1, 0, 0, 0])\n",
    "    True\n",
    "    >>> q = quaternion_from_matrix(numpy.diag([1, -1, -1, 1]))\n",
    "    >>> numpy.allclose(q, [0, 1, 0, 0]) or numpy.allclose(q, [0, -1, 0, 0])\n",
    "    True\n",
    "    >>> R = rotation_matrix(0.123, (1, 2, 3))\n",
    "    >>> q = quaternion_from_matrix(R, True)\n",
    "    >>> numpy.allclose(q, [0.9981095, 0.0164262, 0.0328524, 0.0492786])\n",
    "    True\n",
    "    >>> R = [[-0.545, 0.797, 0.260, 0], [0.733, 0.603, -0.313, 0],\n",
    "    ...      [-0.407, 0.021, -0.913, 0], [0, 0, 0, 1]]\n",
    "    >>> q = quaternion_from_matrix(R)\n",
    "    >>> numpy.allclose(q, [0.19069, 0.43736, 0.87485, -0.083611])\n",
    "    True\n",
    "    >>> R = [[0.395, 0.362, 0.843, 0], [-0.626, 0.796, -0.056, 0],\n",
    "    ...      [-0.677, -0.498, 0.529, 0], [0, 0, 0, 1]]\n",
    "    >>> q = quaternion_from_matrix(R)\n",
    "    >>> numpy.allclose(q, [0.82336615, -0.13610694, 0.46344705, -0.29792603])\n",
    "    True\n",
    "    >>> R = random_rotation_matrix()\n",
    "    >>> q = quaternion_from_matrix(R)\n",
    "    >>> is_same_transform(R, quaternion_matrix(q))\n",
    "    True\n",
    "    >>> is_same_quaternion(quaternion_from_matrix(R, isprecise=False),\n",
    "    ...                    quaternion_from_matrix(R, isprecise=True))\n",
    "    True\n",
    "    >>> R = euler_matrix(0.0, 0.0, numpy.pi/2.0)\n",
    "    >>> is_same_quaternion(quaternion_from_matrix(R, isprecise=False),\n",
    "    ...                    quaternion_from_matrix(R, isprecise=True))\n",
    "    True\n",
    "\n",
    "    \"\"\"\n",
    "    M = np.array(matrix, dtype=np.float64, copy=False)[:4, :4]\n",
    "    if isprecise:\n",
    "        q = np.empty((4, ))\n",
    "        t = np.trace(M)\n",
    "        if t > M[3, 3]:\n",
    "            q[0] = t\n",
    "            q[3] = M[1, 0] - M[0, 1]\n",
    "            q[2] = M[0, 2] - M[2, 0]\n",
    "            q[1] = M[2, 1] - M[1, 2]\n",
    "        else:\n",
    "            i, j, k = 0, 1, 2\n",
    "            if M[1, 1] > M[0, 0]:\n",
    "                i, j, k = 1, 2, 0\n",
    "            if M[2, 2] > M[i, i]:\n",
    "                i, j, k = 2, 0, 1\n",
    "            t = M[i, i] - (M[j, j] + M[k, k]) + M[3, 3]\n",
    "            q[i] = t\n",
    "            q[j] = M[i, j] + M[j, i]\n",
    "            q[k] = M[k, i] + M[i, k]\n",
    "            q[3] = M[k, j] - M[j, k]\n",
    "            q = q[[3, 0, 1, 2]]\n",
    "        q *= 0.5 / math.sqrt(t * M[3, 3])\n",
    "    else:\n",
    "        m00 = M[0, 0]\n",
    "        m01 = M[0, 1]\n",
    "        m02 = M[0, 2]\n",
    "        m10 = M[1, 0]\n",
    "        m11 = M[1, 1]\n",
    "        m12 = M[1, 2]\n",
    "        m20 = M[2, 0]\n",
    "        m21 = M[2, 1]\n",
    "        m22 = M[2, 2]\n",
    "        # symmetric matrix K\n",
    "        K = np.array([[m00-m11-m22, 0.0,         0.0,         0.0],\n",
    "                         [m01+m10,     m11-m00-m22, 0.0,         0.0],\n",
    "                         [m02+m20,     m12+m21,     m22-m00-m11, 0.0],\n",
    "                         [m21-m12,     m02-m20,     m10-m01,     m00+m11+m22]])\n",
    "        K /= 3.0\n",
    "        # quaternion is eigenvector of K that corresponds to largest eigenvalue\n",
    "        w, V = np.linalg.eigh(K)\n",
    "        q = V[[3, 0, 1, 2], np.argmax(w)]\n",
    "    if q[0] < 0.0:\n",
    "        np.negative(q, q)\n",
    "    return q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading Knot_Tying/video/Knot_Tying_B001_capture1.avi\n",
      "total frame count : 1750\n",
      "total labels loaded: 1735\n",
      "(1735, 14)\n",
      "reading Knot_Tying/video/Knot_Tying_B002_capture1.avi\n",
      "total frame count : 1486\n",
      "total labels loaded: 1480\n",
      "(1480, 14)\n",
      "reading Knot_Tying/video/Knot_Tying_B003_capture1.avi\n",
      "total frame count : 1615\n",
      "total labels loaded: 1612\n",
      "(1612, 14)\n",
      "reading Knot_Tying/video/Knot_Tying_B004_capture1.avi\n",
      "total frame count : 1826\n",
      "total labels loaded: 1820\n",
      "(1820, 14)\n",
      "reading Knot_Tying/video/Knot_Tying_C001_capture1.avi\n",
      "total frame count : 1234\n",
      "total labels loaded: 1227\n",
      "(1227, 14)\n",
      "reading Knot_Tying/video/Knot_Tying_C002_capture1.avi\n",
      "total frame count : 1074\n",
      "total labels loaded: 1068\n",
      "(1068, 14)\n",
      "reading Knot_Tying/video/Knot_Tying_C003_capture1.avi\n",
      "total frame count : 1079\n",
      "total labels loaded: 1073\n",
      "(1073, 14)\n",
      "reading Knot_Tying/video/Knot_Tying_C004_capture1.avi\n",
      "total frame count : 1398\n",
      "total labels loaded: 1383\n",
      "(1383, 14)\n",
      "reading Knot_Tying/video/Knot_Tying_C005_capture1.avi\n",
      "total frame count : 1156\n",
      "total labels loaded: 1150\n",
      "(1150, 14)\n",
      "reading Knot_Tying/video/Knot_Tying_D001_capture1.avi\n",
      "total frame count : 1404\n",
      "total labels loaded: 1399\n",
      "(1399, 14)\n",
      "reading Knot_Tying/video/Knot_Tying_D002_capture1.avi\n",
      "total frame count : 1247\n",
      "total labels loaded: 1241\n",
      "(1241, 14)\n",
      "reading Knot_Tying/video/Knot_Tying_D003_capture1.avi\n",
      "total frame count : 1408\n",
      "total labels loaded: 1402\n",
      "(1402, 14)\n",
      "reading Knot_Tying/video/Knot_Tying_D004_capture1.avi\n",
      "total frame count : 1055\n",
      "total labels loaded: 1049\n",
      "(1049, 14)\n",
      "reading Knot_Tying/video/Knot_Tying_D005_capture1.avi\n",
      "total frame count : 1272\n",
      "total labels loaded: 1256\n",
      "(1256, 14)\n",
      "reading Knot_Tying/video/Knot_Tying_E001_capture1.avi\n",
      "total frame count : 1598\n",
      "total labels loaded: 1592\n",
      "(1592, 14)\n",
      "reading Knot_Tying/video/Knot_Tying_E002_capture1.avi\n",
      "total frame count : 1303\n",
      "total labels loaded: 1297\n",
      "(1297, 14)\n",
      "reading Knot_Tying/video/Knot_Tying_E003_capture1.avi\n",
      "total frame count : 1420\n",
      "total labels loaded: 1413\n",
      "(1413, 14)\n",
      "reading Knot_Tying/video/Knot_Tying_E004_capture1.avi\n",
      "total frame count : 1387\n",
      "total labels loaded: 1371\n",
      "(1371, 14)\n",
      "reading Knot_Tying/video/Knot_Tying_E005_capture1.avi\n",
      "total frame count : 1263\n",
      "total labels loaded: 1257\n",
      "(1257, 14)\n",
      "reading Knot_Tying/video/Knot_Tying_F001_capture1.avi\n",
      "total frame count : 926\n",
      "total labels loaded: 920\n",
      "(920, 14)\n",
      "reading Knot_Tying/video/Knot_Tying_F002_capture1.avi\n",
      "total frame count : 1018\n",
      "total labels loaded: 1012\n",
      "(1012, 14)\n",
      "reading Knot_Tying/video/Knot_Tying_F003_capture1.avi\n",
      "total frame count : 2728\n",
      "total labels loaded: 2722\n",
      "(2722, 14)\n",
      "reading Knot_Tying/video/Knot_Tying_F004_capture1.avi\n",
      "total frame count : 1836\n",
      "total labels loaded: 1830\n",
      "(1830, 14)\n",
      "reading Knot_Tying/video/Knot_Tying_F005_capture1.avi\n",
      "total frame count : 1380\n",
      "total labels loaded: 1849\n",
      "(1379, 14)\n",
      "reading Knot_Tying/video/Knot_Tying_G001_capture1.avi\n",
      "total frame count : 2374\n",
      "total labels loaded: 2369\n",
      "(2369, 14)\n",
      "reading Knot_Tying/video/Knot_Tying_G002_capture1.avi\n",
      "total frame count : 2287\n",
      "total labels loaded: 2282\n",
      "(2282, 14)\n",
      "reading Knot_Tying/video/Knot_Tying_G003_capture1.avi\n",
      "total frame count : 2429\n",
      "total labels loaded: 2415\n",
      "(2415, 14)\n",
      "reading Knot_Tying/video/Knot_Tying_G004_capture1.avi\n",
      "total frame count : 3867\n",
      "total labels loaded: 3853\n",
      "(3853, 14)\n",
      "reading Knot_Tying/video/Knot_Tying_G005_capture1.avi\n",
      "total frame count : 1701\n",
      "total labels loaded: 1696\n",
      "(1696, 14)\n",
      "reading Knot_Tying/video/Knot_Tying_H003_capture1.avi\n",
      "total frame count : 1641\n",
      "total labels loaded: 1635\n",
      "(1635, 14)\n",
      "reading Knot_Tying/video/Knot_Tying_H004_capture1.avi\n",
      "total frame count : 2256\n",
      "total labels loaded: 2251\n",
      "(2251, 14)\n",
      "reading Knot_Tying/video/Knot_Tying_H005_capture1.avi\n",
      "total frame count : 1958\n",
      "total labels loaded: 1952\n",
      "(1952, 14)\n",
      "reading Knot_Tying/video/Knot_Tying_I001_capture1.avi\n",
      "total frame count : 2335\n",
      "total labels loaded: 2329\n",
      "(2329, 14)\n",
      "reading Knot_Tying/video/Knot_Tying_I002_capture1.avi\n",
      "total frame count : 2433\n",
      "total labels loaded: 2426\n",
      "(2426, 14)\n",
      "reading Knot_Tying/video/Knot_Tying_I003_capture1.avi\n",
      "total frame count : 2070\n",
      "total labels loaded: 2063\n",
      "(2063, 14)\n",
      "reading Knot_Tying/video/Knot_Tying_I005_capture1.avi\n",
      "total frame count : 2661\n",
      "total labels loaded: 2655\n",
      "(2655, 14)\n",
      "(61614, 14)\n"
     ]
    }
   ],
   "source": [
    "subject_list = {}\n",
    "subject_list['KT'] = [['B','C','D','E','F','G','H','I'],[[0,1,2,3],[0,1,2,3,4],[0,1,2,3,4],[0,1,2,3,4],[0,1,2,3,4],[0,1,2,3,4],[2,3,4],[0,1,2,4]]]\n",
    "\n",
    "task = 'KT'\n",
    "act = 'Knot_Tying'\n",
    "subj = subject_list[task][0]\n",
    "trial = subject_list[task][1]\n",
    "\n",
    "y = np.empty((0,14)) # 6 for position, 24 for direction cosines, 14 for quaternion\n",
    "\n",
    "index_dict = {}\n",
    "\n",
    "for i in range(len(subj)):\n",
    "    for j in trial[i]:\n",
    "        # collect kinematic data\n",
    "        count = collect_video_framecount(task,subj[i],j+1)\n",
    "        filepath = act + '/kinematics/AllGestures/'\n",
    "        filename = filepath + act + '_' + subj[i] + '00' + str(j+1) + '.txt'\n",
    "        data = np.loadtxt(filename)\n",
    "        \n",
    "        r_test = data[0, 3:12]\n",
    "        \n",
    "        num_labels = data.shape[0]\n",
    "        print('total labels loaded: %d' % num_labels)\n",
    "        dataL_pos = data[:count,39:42]\n",
    "        dataR_pos = data[:count,57:60]\n",
    "        \n",
    "        dataL_rot = data[:count, 41:50]\n",
    "        dataR_rot = data[:count, 60:69]\n",
    "        \n",
    "        # now we change the representation of the rotation to quaternion\n",
    "        N = dataL_rot.shape[0]\n",
    "        dataL_quat = np.zeros((N, 4))\n",
    "        dataR_quat = np.zeros((N, 4))\n",
    "        for k in range(N):\n",
    "            L_rot = np.asarray(dataL_rot[k, :]).reshape((3,3))\n",
    "            R_rot = np.asarray(dataR_rot[k, :]).reshape((3,3))\n",
    "\n",
    "            dataL_quat[k,:] = quaternion_from_matrix(L_rot)\n",
    "            dataR_quat[k,:] = quaternion_from_matrix(R_rot)\n",
    "        \n",
    "        dataL = np.hstack((dataL_pos, dataL_quat))\n",
    "        dataR = np.hstack((dataR_pos, dataR_quat))\n",
    "        out = np.hstack((dataL,dataR))\n",
    "        print(out.shape)\n",
    "        \n",
    "        index_dict[subj[i] + '00' + str(j+1)] = list(range(y.shape[0], y.shape[0] + out.shape[0]))\n",
    "        \n",
    "        y = np.vstack((y,out))\n",
    "import pickle\n",
    "\n",
    "print(y.shape)\n",
    "picklefile = open('kinematics', 'wb') \n",
    "pickle.dump(y,picklefile)\n",
    "picklefile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61614\n",
      "(61614, 14)\n",
      "using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# functions to downsize the images and compile them into a dataset\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import sampler\n",
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as T\n",
    "import glob\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "import pdb\n",
    "\n",
    "import random\n",
    "\n",
    "class JIGSAWDataset(Dataset):\n",
    "\n",
    "    def __init__(self, y, root_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.labels = y\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.y.shape[0]\n",
    "        \n",
    "    def __getitem__(self, idx): \n",
    "        file_list = glob.glob('data/*.png')\n",
    "        img_name = file_list[idx]\n",
    "        #print('opening image ' +  img_name)\n",
    "        image = Image.open(img_name,'r')\n",
    "        label = self.labels[idx,:]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        sample =  (image,label)\n",
    "\n",
    "        return sample\n",
    "\n",
    "def load_dataset():\n",
    "    \n",
    "    data_path = 'data'\n",
    "    picklefile = open(\"kinematics\", \"rb\" )\n",
    "    \n",
    "    num_files = len(next(os.walk('data'))[2]) #dir is your directory path as string\n",
    "    print(num_files)\n",
    "    \n",
    "    trans_height = 10 \n",
    "    trans_width = 15\n",
    "    \n",
    "    trans = T.Compose([\n",
    "                T.Resize((trans_height,trans_width), interpolation=2),\n",
    "                T.ToTensor()])\n",
    "    transy = T.Compose([T.ToTensor()])\n",
    "\n",
    "    y = pickle.load(picklefile)\n",
    "    print(y.shape)\n",
    "    picklefile.close()\n",
    "    \n",
    "    dataset = JIGSAWDataset(y,data_path,transform = trans)\n",
    "    \n",
    "    num_train_trials = 3\n",
    "    num_val_trials = 1\n",
    "        \n",
    "    range_total = np.asarray(random.sample(list(index_dict.values()), k=(num_train_trials + num_val_trials)))\n",
    "    range_train = list(range_total[:num_train_trials].flatten())[0]\n",
    "    range_val = list(range_total[num_train_trials:num_train_trials+num_val_trials].flatten())[0]\n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=25,\n",
    "        num_workers=0,\n",
    "        shuffle=False,\n",
    "#         sampler=sampler.SubsetRandomSampler(range(num_files-2000))\n",
    "        sampler=sampler.SubsetRandomSampler(range_train)\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=25,\n",
    "        num_workers=0,\n",
    "        shuffle=False,\n",
    "#         sampler=sampler.SubsetRandomSampler(range(num_files-2000,num_files))\n",
    "        sampler=sampler.SubsetRandomSampler(range_val)\n",
    "    )\n",
    "    \n",
    "    return train_loader,val_loader\n",
    "    \n",
    "loader_train,loader_val = load_dataset()\n",
    "\n",
    "\n",
    "USE_GPU = True\n",
    "dtype = torch.float32 # we will be using float throughout this tutorial\n",
    "\n",
    "if USE_GPU and torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "# Constant to control how frequently we print train loss\n",
    "print_every = 100\n",
    "\n",
    "print('using device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_accuracy_part34(loader, model):\n",
    "\n",
    "    print('Checking accuracy on validation set')\n",
    "\n",
    "    model.eval()  # set model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device=device, dtype=dtype)  # move to device, e.g. GPU\n",
    "            y = y.to(device=device, dtype=torch.float)\n",
    "            scores = model(x)\n",
    "            loss_fn = torch.nn.MSELoss(reduction='mean')\n",
    "            loss = loss_fn(scores,y)\n",
    "\n",
    "        print('MSE loss is: %d ' % loss)\n",
    "\n",
    "def train_part34(model, optimizer, epochs=1):\n",
    "    \"\"\"\n",
    "    Train a model on CIFAR-10 using the PyTorch Module API.\n",
    "    \n",
    "    Inputs:\n",
    "    - model: A PyTorch Module giving the model to train.\n",
    "    - optimizer: An Optimizer object we will use to train the model\n",
    "    - epochs: (Optional) A Python integer giving the number of epochs to train for\n",
    "    \n",
    "    Returns: Nothing, but prints model accuracies during training.\n",
    "    \"\"\"\n",
    "    model = model.to(device=device)  # move the model parameters to CPU/GPU\n",
    "    for e in range(epochs):\n",
    "        for t, (x, y) in enumerate(loader_train):\n",
    "            model.train()  # put model to training mode\n",
    "            x = x.to(device=device, dtype=dtype)  # move to device, e.g. GPU\n",
    "            y = y.to(device=device, dtype=torch.float)\n",
    "            \n",
    "            scores = model(x)\n",
    "            loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "            loss = loss_fn(scores,y)\n",
    "\n",
    "            # Zero out all of the gradients for the variables which the optimizer\n",
    "            # will update.\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # This is the backwards pass: compute the gradient of the loss with\n",
    "            # respect to each  parameter of the model.\n",
    "            loss.backward()\n",
    "\n",
    "            # Actually update the parameters of the model using the gradients\n",
    "            # computed by the backwards pass.\n",
    "            optimizer.step()\n",
    "            \n",
    "            if t % 5 == 0:\n",
    "                print('Iteration %d, loss = %.4f' % (t, loss.item()))\n",
    "                #check_accuracy_part34(loader_train, model)\n",
    "                print()\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15 10\n",
      "7 4\n",
      "4 3\n",
      "linear input dim required:  936\n",
      "Iteration 0, loss = 84.2824\n",
      "\n",
      "Iteration 5, loss = 74.1670\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def flatten(x):\n",
    "    N = x.shape[0] # read in N, C, H, W\n",
    "    return x.view(N, -1)  # \"flatten\" the C * H * W values into a single vector per image\n",
    "\n",
    "class Flatten(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return flatten(x)\n",
    "\n",
    "model = None\n",
    "optimizer = None\n",
    "\n",
    "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "channel_1 = 16\n",
    "channel_2 = 32\n",
    "channel_3 = 64\n",
    "hidden_size = 1000\n",
    "learning_rate = 1e-3\n",
    "reg = 1e-5\n",
    "\n",
    "# computing the output from the conv nets\n",
    "pad1 = 2 \n",
    "filt1 = 5\n",
    "im_H = 10\n",
    "im_W = 15\n",
    "stride1 = 1\n",
    "wout1 = (im_W + 2*pad1 - filt1 )/stride1 + 1\n",
    "hout1 = (im_H + 2*pad1 - filt1 )/stride1 + 1\n",
    "print('%d %d' %(wout1,hout1))\n",
    "pad2 = 1\n",
    "filt2 = 5\n",
    "stride2 =2\n",
    "wout2 = (wout1 + 2 *pad2 - filt2 )/stride2 + 1\n",
    "hout2 = (hout1 + 2 *pad2 - filt2 )/stride2 + 1\n",
    "print('%d %d' %(wout2,hout2))\n",
    "pad3 = 1\n",
    "filt3 = 2\n",
    "stride3 = 2\n",
    "wout3 = (wout2 + 2 *pad3 - filt3 )/stride3 + 1\n",
    "hout3 = (hout2 + 2 *pad3 - filt3 )/stride3 + 1\n",
    "print('%d %d' %(wout3,hout3))\n",
    "linear_input_dim = int(channel_3*hout3*wout3)\n",
    "print('linear input dim required: ' ,linear_input_dim)\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Conv2d(3,channel_1,filt1,padding=pad1,stride = stride1),\n",
    "    nn.BatchNorm2d(channel_1),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(channel_1,channel_2,filt2,padding=pad2, stride = stride2),\n",
    "    nn.BatchNorm2d(channel_2),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(channel_2,channel_3,filt3,padding=pad3,stride = stride3),\n",
    "    nn.BatchNorm2d(channel_3),\n",
    "    nn.ReLU(),\n",
    "    Flatten(),\n",
    "#     nn.Linear(19200,hidden_size),\n",
    "    nn.Linear(768,hidden_size),\n",
    "    nn.BatchNorm1d(hidden_size),\n",
    "    nn.ReLU(),\n",
    "#     nn.Linear(hidden_size,6),\n",
    "    nn.Linear(hidden_size,14),\n",
    ")\n",
    "\n",
    "# you can use Nesterov momentum in optim.SGD\n",
    "#optimizer = optim.SGD(model.parameters(), lr=learning_rate,momentum=0.9, nesterov=True)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(),lr = learning_rate, weight_decay = reg)\n",
    "\n",
    "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "################################################################################\n",
    "#                                 END OF YOUR CODE                             \n",
    "################################################################################\n",
    "\n",
    "# You should get at least 70% accuracy\n",
    "train_part34(model, optimizer, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data_path = 'data'\n",
    "picklefile = open(\"kinematics\", \"rb\" )\n",
    "\n",
    "num_files = len(next(os.walk('data'))[2]) #dir is your directory path as string\n",
    "print(num_files)\n",
    "\n",
    "\n",
    "\n",
    "trans = T.Compose([\n",
    "            T.Resize((60,80), interpolation=2),\n",
    "            T.ToTensor()])\n",
    "transy = T.Compose([T.ToTensor()])\n",
    "transr = T.Compose([T.Resize((30,40), interpolation=2)])\n",
    "\n",
    "image = Image.open('data/G_5_1_918_KT.png','r')\n",
    "plt.imshow(image)\n",
    "plt.figure()\n",
    "image = transr(image)\n",
    "plt.imshow(image)\n",
    "\n",
    "'''\n",
    "y = pickle.load(picklefile)\n",
    "picklefile.close()\n",
    "\n",
    "dataset = JIGSAWDataset(y,data_path,transform = trans)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
